{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "0NyHiQe2u1-N",
        "uHCHhbbFu8mm",
        "vH6wJLE_vCFt",
        "KSWnePjvvHOq",
        "-BF2lUB0vNny"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgz22QFVf0Bd"
      },
      "outputs": [],
      "source": [
        "import imaplib\n",
        "import email\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bs4 import BeautifulSoup\n",
        "import smtplib\n",
        "from email.message import EmailMessage\n",
        "from email.utils import parsedate_to_datetime\n",
        "from email.header import decode_header\n",
        "import os\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PhishingDetector(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(PhishingDetector, self).__init__()\n",
        "\n",
        "#Layer 1: 256 neurons\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.bn1 = nn.BatchNorm1d(256)\n",
        "        self.dropout1 = nn.Dropout(0.4)\n",
        "\n",
        "#Layer 2: 128 neurons\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "\n",
        "#Layer 3: 64 neurons\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.bn3 = nn.BatchNorm1d(64)\n",
        "        self.dropout3 = nn.Dropout(0.3)\n",
        "\n",
        "#Layer 4: 32 neurons\n",
        "        self.fc4 = nn.Linear(64, 32)\n",
        "        self.bn4 = nn.BatchNorm1d(32)\n",
        "        self.dropout4 = nn.Dropout(0.2)\n",
        "\n",
        "#Output layer\n",
        "        self.fc5 = nn.Linear(32, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer 1\n",
        "        x = self.fc1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "#Layer 2\n",
        "        x = self.fc2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "#Layer 3\n",
        "        x = self.fc3(x)\n",
        "        x = self.bn3(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "#Layer 4\n",
        "        x = self.fc4(x)\n",
        "        x = self.bn4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout4(x)\n",
        "\n",
        "#Output\n",
        "        x = self.fc5(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "A346IEnIA6Av"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EMAIL = os.environ.get(\"PHISHING_EMAIL\")\n",
        "APP_PASSWORD = os.environ.get(\"PHISHING_APP_PASSWORD\")\n",
        "N = 5\n"
      ],
      "metadata": {
        "id": "QxI3IXqff2Ld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The Helper functions"
      ],
      "metadata": {
        "id": "0NyHiQe2u1-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_header(msg, name):\n",
        "    return msg.get(name, \"\")\n",
        "\n",
        "def clean_html(html):\n",
        "    soup = BeautifulSoup(html, \"html.parser\")\n",
        "    for tag in soup([\"script\", \"style\"]):\n",
        "        tag.decompose()\n",
        "    return \" \".join(soup.get_text(separator=\" \").split())\n",
        "\n",
        "def extract_links(text):\n",
        "    return re.findall(r'https?://[^\\s]+', text)\n",
        "\n",
        "def decode_mime_header(value):\n",
        "    if not value:\n",
        "        return \"\"\n",
        "\n",
        "    decoded_parts = decode_header(value)\n",
        "    decoded_string = \"\"\n",
        "\n",
        "    for part, encoding in decoded_parts:\n",
        "        if isinstance(part, bytes):\n",
        "            try:\n",
        "                decoded_string += part.decode(encoding or \"utf-8\", errors=\"ignore\")\n",
        "            except Exception:\n",
        "                decoded_string += part.decode(\"utf-8\", errors=\"ignore\")\n",
        "        else:\n",
        "            decoded_string += part\n",
        "\n",
        "    return decoded_string.strip()\n",
        "\n"
      ],
      "metadata": {
        "id": "8wHkpLb4f54B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Read Email"
      ],
      "metadata": {
        "id": "uHCHhbbFu8mm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "mail = imaplib.IMAP4_SSL(\"imap.gmail.com\")\n",
        "mail.login(EMAIL, APP_PASSWORD)\n",
        "mail.select(\"inbox\")\n",
        "\n",
        "status, messages = mail.search(None, 'UNSEEN')\n",
        "email_ids = messages[0].split()[-N:]\n",
        "\n",
        "emails_data = []\n",
        "\n",
        "for e_id in email_ids:\n",
        "    _, msg_data = mail.fetch(e_id, \"(RFC822)\")\n",
        "    msg = email.message_from_bytes(msg_data[0][1])\n",
        "\n",
        "    subject = decode_mime_header(get_header(msg, \"Subject\"))\n",
        "    sender  = decode_mime_header(get_header(msg, \"From\"))\n",
        "    to      = decode_mime_header(get_header(msg, \"To\"))\n",
        "    cc      = decode_mime_header(get_header(msg, \"Cc\"))\n",
        "    bcc     = decode_mime_header(get_header(msg, \"Bcc\"))\n",
        "\n",
        "    body = \"\"\n",
        "\n",
        "    if msg.is_multipart():\n",
        "        for part in msg.walk():\n",
        "            payload = part.get_payload(decode=True)\n",
        "            if payload:\n",
        "                text = payload.decode(errors=\"ignore\")\n",
        "                if part.get_content_type() == \"text/plain\":\n",
        "                    body += text\n",
        "                elif part.get_content_type() == \"text/html\":\n",
        "                    body += clean_html(text)\n",
        "    else:\n",
        "        payload = msg.get_payload(decode=True)\n",
        "        if payload:\n",
        "            body = clean_html(payload.decode(errors=\"ignore\"))\n",
        "\n",
        "    urls = extract_links(body)\n",
        "\n",
        "    emails_data.append({\n",
        "        \"subject\": subject,\n",
        "        \"from\": sender,\n",
        "        \"to\": to,\n",
        "        \"cc\": cc,\n",
        "        \"bcc\": bcc,\n",
        "        \"body\": body,\n",
        "        \"date\": get_header(msg, \"Date\"),\n",
        "        \"url\": \" \".join(urls)\n",
        "    })\n",
        "\n",
        "    mail.store(e_id, '+FLAGS', '\\\\Seen')\n",
        "\n",
        "df = pd.DataFrame(emails_data)\n"
      ],
      "metadata": {
        "id": "Sy6s035ff_69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Keyword lists"
      ],
      "metadata": {
        "id": "vH6wJLE_vCFt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "URGENT_KEYWORDS = [\n",
        "    \"urgent\" , \"urgently\" , \"immediately\" , \"immediate\" , \"immediate action\" ,\n",
        "    \"respond immediately\" , \"important\" , \"high importance\" , \"high priority\" ,\n",
        "    \"priority\" , \"action required\" , \"act now\" , \"respond now\" , \"asap\" ,\n",
        "    \"verify now\" , \"confirm now\" , \"update now\" , \"review now\" ,\n",
        "    \"expire\" , \"expires\" , \"expires today\" , \"expiring soon\" , \"last warning\" ,\n",
        "    \"limited time\" , \"within 24 hours\" , \"within 48 hours\" , \"within hours\" ,\n",
        "    \"time is running out\" , \"offer expires\" , \"today only\" ,\n",
        "    \"urgent response needed\" , \"critical update\" , \"attention required\" ,\n",
        "    \"final reminder\" , \"final warning\" , \"immediate response needed\" ,\n",
        "    \"deadline\" , \"due today\" , \"due now\" , \"time-sensitive\" , \"time sensitive\" ,\n",
        "    \"respond asap\" , \"quickly\" , \"fast action required\" , \"take action now\" ,\n",
        "    \"do this immediately\" , \"important notice\" , \"emergency action\" ,\n",
        "    \"critical issue\" , \"must act now\" , \"respond without delay\" ,\n",
        "    \"urgent attention\" , \"requires immediate action\"\n",
        "]\n",
        "\n",
        "THREAT_KEYWORDS = [\n",
        "    \"suspend\" , \"suspended\" , \"locked\" , \"locked out\" , \"close your account\" ,\n",
        "    \"account closure\" , \"legal action\" , \"legal notice\" , \"final notice\" ,\n",
        "    \"termination\" , \"deactivate\" , \"deactivation\" , \"restricted\" , \"restriction\" ,\n",
        "    \"blocked\" , \"frozen\" , \"compromised\" , \"breach\" , \"security breach\" ,\n",
        "    \"hacked\" , \"unauthorized\" , \"unauthorized access\" , \"unusual activity\" ,\n",
        "    \"violation\" , \"policy violation\" , \"will be closed\" , \"forced closure\" ,\n",
        "    \"take action or\" , \"lose access\" , \"permanently removed\" ,\n",
        "    \"account disabled\" , \"security suspension\" , \"threat detected\" ,\n",
        "    \"security risk\" , \"security warning\" , \"final attempt\" , \"failure to respond\" ,\n",
        "    \"account will be terminated\" , \"forced deactivation\" , \"breach detected\" ,\n",
        "    \"critical security issue\" , \"account compromised\" , \"service interruption\" ,\n",
        "    \"security alert\" , \"high-risk login\" , \"multiple failed attempts\"\n",
        "]\n",
        "\n",
        "MONEY_PRIZE_KEYWORDS = [\n",
        "    \"win\" , \"winner\" , \"won\" , \"prize\" , \"grand prize\" , \"cash\" , \"cash prize\" ,\n",
        "    \"reward\" , \"bonus\" , \"lottery\" , \"jackpot\" , \"million\" , \"million dollars\" ,\n",
        "    \"refund\" , \"tax refund\" , \"tax rebate\" , \"rebate\" , \"unclaimed\" ,\n",
        "    \"compensation\" , \"inheritance\" , \"beneficiary\" , \"grant\" , \"funds\" ,\n",
        "    \"payment released\" , \"claim your\" , \"claim now\" , \"redeem now\" ,\n",
        "    \"congratulations you\" , \"free money\" , \"you've been selected\" ,\n",
        "    \"$$$\" , \"financial award\" , \"monetary reward\" , \"payout\" , \"transfer\" ,\n",
        "    \"urgent refund\" , \"deposit available\" , \"funds available\" ,\n",
        "    \"cash transfer\" , \"unexpected funds\" , \"reward waiting\" , \"prize awaiting\" ,\n",
        "    \"selected as winner\" , \"lucky draw\" , \"special payout\" ,\n",
        "    \"exclusive reward\" , \"free bonus\" , \"instant winnings\"\n",
        "]\n",
        "\n",
        "URGENCY_PHRASES = [\n",
        "    \"act now\" , \"don't wait\" , \"hurry\" , \"last chance\" , \"time is running out\" ,\n",
        "    \"offer expires\" , \"today only\" , \"now or never\" , \"don't miss out\" ,\n",
        "    \"limited offer\" , \"limited-time\" , \"while supplies last\" ,\n",
        "    \"urgent response needed\" , \"expires soon\" , \"limited time\" , \"dont miss\" ,\n",
        "    \"final opportunity\" , \"respond quickly\" , \"immediate attention required\" ,\n",
        "    \"before itâ€™s too late\" , \"claim before expiry\" ,\n",
        "    \"offer ends soon\" , \"only hours left\" , \"only today\" ,\n",
        "    \"limited quantity\" , \"final hours\" , \"respond before deadline\"\n",
        "]\n",
        "\n",
        "GENERIC_SUSPICIOUS = [\n",
        "    \"click here\" , \"click below\" , \"click link\" , \"open link\" , \"login\" ,\n",
        "    \"log in\" , \"update\" , \"update account\" , \"verify\" , \"verify your\" ,\n",
        "    \"confirm\" , \"confirm your\" , \"validate\" , \"review\" , \"reactivate\" ,\n",
        "    \"re-activate\" , \"password\" , \"security alert\" , \"dear customer\" ,\n",
        "    \"dear user\" , \"valued customer\" , \"account holder\" , \"update payment\" ,\n",
        "    \"billing information\" , \"payment method\" , \"credit card\" , \"card details\" ,\n",
        "    \"reset password\" , \"identity verification\" , \"authentication required\" ,\n",
        "    \"verify immediately\" , \"important update\" , \"account review\" ,\n",
        "    \"confirm identity\" , \"provide details\" , \"submit information\" ,\n",
        "    \"resolve issue\" , \"confirm account\" , \"login required\" ,\n",
        "    \"reset your account\" , \"unlock account\" , \"secure login\" ,\n",
        "    \"update credentials\" , \"verification process\" , \"restore access\" ,\n",
        "    \"protect your account\" , \"account verification\"\n",
        "]\n",
        "\n",
        "COMPANY_IMPERSONATION = [\n",
        "    \"paypal\" , \"amazon\" , \"apple\" , \"microsoft\" , \"google\" , \"facebook\" ,\n",
        "    \"instagram\" , \"twitter\" , \"linkedin\" , \"bank\" , \"irs\" , \"fedex\" , \"usps\" ,\n",
        "    \"ups\" , \"dhl\" , \"netflix\" , \"ebay\" , \"ssa\" , \"social security\" ,\n",
        "    \"wells fargo\" , \"chase\" , \"bank of america\" , \"citibank\" , \"hsbc\" ,\n",
        "    \"capital one\" , \"barclays\" , \"royal mail\" , \"revolut\" , \"stripe\" ,\n",
        "    \"coinbase\" , \"binance\" , \"mcdonalds\" , \"spotify\" , \"icloud\" ,\n",
        "    \"adobe\" , \"dropbox\" , \"onedrive\" , \"office365\" , \"outlook\" , \"teams\" ,\n",
        "    \"zoho\" , \"intuit\" , \"quickbooks\" ,\n",
        "    \"amazon support\" , \"microsoft support\" , \"google security\" ,\n",
        "    \"bank security\" , \"visa\" , \"mastercard\" , \"amex\" ,\n",
        "    \"national lottery\" , \"telecom\" , \"azure\" , \"aws\" , \"gmail team\" ,\n",
        "    \"facebook security\"\n",
        "]\n",
        "\n",
        "SPOOFING_INDICATORS = [\n",
        "    \"noreply\" , \"no-reply\" , \"donotreply\" , \"do-not-reply\" ,\n",
        "    \"support@\" , \"admin@\" , \"security@\" , \"verification@\" , \"alert@\" ,\n",
        "    \"service@\" , \"info@\" , \"notification@\" , \"update@\" , \"mailer@\" , \"robot@\" ,\n",
        "    \"support-team@\" , \"supportdesk@\" , \"helpdesk@\" , \"accounts@\" ,\n",
        "    \"billing@\" , \"customerservice@\" , \"compliance@\" , \"system@\" , \"system-mail@\" ,\n",
        "    \"alerts@\" , \"noreplymail@\" , \"auto-mailer@\" , \"webmaster@\" , \"it-support@\"\n",
        "]\n",
        "\n",
        "CREDENTIAL_REQUESTS = [\n",
        "    \"enter your password\" , \"provide your password\" , \"confirm password\" ,\n",
        "    \"password\" , \"username and password\" , \"username\" , \"user id\" ,\n",
        "    \"social security number\" , \"ssn\" , \"account number\" , \"credit card\" ,\n",
        "    \"credit card number\" , \"card details\" , \"cvv\" , \"pin number\" , \"pin\" ,\n",
        "    \"date of birth\" , \"dob\" , \"mother's maiden name\" , \"security question\" ,\n",
        "    \"full name and address\" , \"bank details\" , \"routing number\" ,\n",
        "    \"id card\" , \"passport number\" , \"driver license\" , \"two-factor code\" ,\n",
        "    \"otp\" , \"one-time password\" , \"authentication code\" , \"verify identity\" ,\n",
        "    \"enter your credentials\" , \"provide login details\" , \"reauthenticate\" ,\n",
        "    \"input verification code\" , \"enter 2fa code\" , \"banking password\" ,\n",
        "    \"enter digits\" , \"identity confirmation\" , \"submit credentials\"\n",
        "]\n",
        "\n",
        "FREE_EMAIL_DOMAINS = [\n",
        "    \"gmail.com\" , \"yahoo.com\" , \"hotmail.com\" , \"outlook.com\" , \"aol.com\" ,\n",
        "    \"mail.com\" , \"protonmail.com\" , \"yandex.com\" , \"gmx.com\" , \"icloud.com\" ,\n",
        "    \"live.com\" , \"msn.com\" , \"inbox.com\" , \"fastmail.com\" , \"zoho.com\" ,\n",
        "    \"hushmail.com\" , \"tutanota.com\" , \"yahoo.co.uk\" , \"outlook.co.uk\" ,\n",
        "    \"hotmail.co.uk\" , \"googlemail.com\" , \"mail.ru\" , \"gmx.net\" ,\n",
        "    \"yandex.ru\" , \"usa.com\" , \"europe.com\"\n",
        "]\n",
        "\n",
        "personal_pronouns = [\n",
        "    \" i \" , \" me \" , \" myself \" , \" my \" , \" mine \" ,\n",
        "    \" you \" , \" your \" , \" yours \" , \" yourself \" , \" yourselves \" ,\n",
        "    \" we \" , \" us \" , \" our \" , \" ours \" , \" ourselves \" ,\n",
        "    \" he \" , \" him \" , \" his \" , \" himself \" ,\n",
        "    \" she \" , \" her \" , \" hers \" , \" herself \" ,\n",
        "    \" they \" , \" them \" , \" their \" , \" theirs \" , \" themselves \" ,\n",
        "    \" i'm \" , \" you're \" , \" we're \" , \" they're \" ,\n",
        "    \" i'd \" , \" you'd \" , \" we'd \" , \" they'd \" ,\n",
        "    \" i'll \" , \" you'll \" , \" we'll \" , \" they'll \" ,\n",
        "    \" i've \" , \" you've \" , \" we've \" , \" they've \" ,\n",
        "    \" u \" , \" ur \" , \" im \" , \" id \" , \" youll \" , \" weve \" ,\n",
        "    \" ya \" , \" u r \" , \" u're \"\n",
        "]\n",
        "\n",
        "action_words = [\n",
        "    \"click here\" , \"verify now\" , \"update now\" , \"confirm now\" , \"act now\" ,\n",
        "    \"login\" , \"log in\" , \"sign in\" , \"sign-in\" , \"reset password\" ,\n",
        "    \"unlock account\" , \"reactivate account\" , \"open attachment\" ,\n",
        "    \"open file\" , \"download\" , \"run file\" , \"install\" , \"enable macros\" ,\n",
        "    \"enable content\" , \"open this link\" , \"visit link\" , \"review document\" ,\n",
        "    \"access portal\" , \"complete form\" , \"submit info\" , \"submit information\" ,\n",
        "    \"provide details\" , \"authorize\" , \"approve request\" , \"authenticate\" ,\n",
        "    \"verify identity\" , \"take action\" , \"respond now\" , \"reply now\" ,\n",
        "    \"urgent action required\" , \"redeem now\" , \"claim reward\" , \"claim prize\" ,\n",
        "    \"claim bonus\" , \"authenticate now\" , \"fix account\" , \"resolve issue\" ,\n",
        "    \"update details\" , \"check status\" , \"check your account\" , \"security check\" ,\n",
        "    \"identity check\" , \"invoice due\" , \"payment required\" , \"confirm payment\" ,\n",
        "    \"review invoice\" , \"download invoice\" , \"open invoice\" , \"final warning\" ,\n",
        "    \"last notice\" , \"immediate attention\" , \"time-sensitive\" , \"expires today\" ,\n",
        "    \"expires soon\" , \"within 24 hours\" , \"within 48 hours\" , \"verify details\" ,\n",
        "    \"confirm identity\" , \"update credentials\" , \"reactivate immediately\" ,\n",
        "    \"urgent verification\" , \"review activity\" ,\n",
        "    \"review payment\" , \"open secure message\" , \"open secure file\" ,\n",
        "    \"download secure document\" , \"complete verification\" ,\n",
        "    \"follow instructions\" , \"press the button\" , \"tap to verify\" ,\n",
        "    \"accept request\" , \"approve payment\" , \"review balance\"\n",
        "]\n",
        "\n",
        "attachment_keywords = [\n",
        "    \"attached\" , \"attachment\" , \"find attached\" , \"see attached\" , \"enclosed\" ,\n",
        "    \"included\" , \"attached file\" , \"attached document\" , \"attached invoice\" ,\n",
        "    \"file\" , \"document\" , \"invoice\" , \"statement\" , \"report\" , \"form\" , \"pdf\" ,\n",
        "    \"doc\" , \"docx\" , \"xls\" , \"xlsx\" , \"ppt\" , \"pptx\" , \"zip\" , \"rar\" , \"7z\" , \"tar\" ,\n",
        "    \"gz\" , \"invoice.pdf\" , \"statement.pdf\" , \"payment.docx\" , \"document.pdf\" ,\n",
        "    \"remittance.pdf\" , \"receipt.pdf\" , \"scan.pdf\" , \"scanned document\" ,\n",
        "    \"secure document\" , \"protected file\" , \"important file\" ,\n",
        "    \"download attachment\" , \"download file\" , \"open document\" , \"open report\" ,\n",
        "    \"review attachment\" , \"package attached\" ,\n",
        "    \"secure pdf\" , \"encrypted file\" , \"password-protected file\" ,\n",
        "    \"confidential document\" , \"urgent document\" , \"delivery note\" ,\n",
        "    \"shipping label\" , \"invoice copy\" , \"wire details\" , \"account statement\"\n",
        "]\n",
        "\n",
        "common_misspellings = [\n",
        "    \"urgnet\" , \"accout\" , \"pasword\" , \"verifiy\" , \"confirim\" , \"securty\" ,\n",
        "    \"recieve\" , \"bussiness\" , \"offical\" , \"adress\" , \"addres\" , \"identiy\" ,\n",
        "    \"identitiy\" , \"authenticaion\" , \"updatte\" , \"verfication\" , \"verificatoin\" ,\n",
        "    \"verifiction\" , \"logn\" , \"loggin\" , \"loggin in\" , \"passwrod\" , \"paswrod\" ,\n",
        "    \"pssword\" , \"confrim\" , \"confim\" , \"confurm\" , \"securrity\" , \"securitty\" ,\n",
        "    \"acount\" , \"acoount\" , \"accuont\" , \"accoubt\" , \"acc0unt\" , \"passw0rd\" ,\n",
        "    \"ver1fy\" , \"conf1rm\" , \"secur1ty\" , \"verlfy\" , \"veriry\" , \"authentlcate\" ,\n",
        "    \"l0gin\" , \"resp0nd\" , \"paymnet\" , \"invocie\" , \"docuemnt\" , \"statment\" ,\n",
        "    \"recipt\" , \"notcie\" , \"activatoin\" , \"restircted\" , \"suspened\" , \"prizee\" ,\n",
        "    \"reawrd\" , \"bannk\" , \"accuont\" , \"micorsoft\" , \"microsofft\" ,\n",
        "    \"protction\" , \"verificaiton\" , \"confurmation\" , \"identificatoin\" ,\n",
        "    \"autheticatoin\" ,\n",
        "    \"passowrd\" , \"passworld\" , \"loggin\" , \"verifcation\" , \"immediatly\" ,\n",
        "    \"urjent\" , \"supension\" , \"restrction\" , \"invoie\" , \"documant\" ,\n",
        "    \"statemant\" , \"bankk\" , \"paymnent\" , \"invocie\" , \"accpunt\" ,\n",
        "    \"vaccant\" , \"authentcation\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "grGHw_rSgUad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##parse date text function"
      ],
      "metadata": {
        "id": "KSWnePjvvHOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def parse_date_text(date_str):\n",
        "    month_map = {\n",
        "        'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n",
        "        'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12\n",
        "    }\n",
        "\n",
        "    weekdays = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
        "\n",
        "    date_str = str(date_str).strip().lower()\n",
        "\n",
        "    # 1 - Original logic (PRIMARY)\n",
        "    weekday = day = month = year = hour = minute = second = None\n",
        "\n",
        "    # Weekday\n",
        "    if len(date_str) >= 3:\n",
        "        weekday_str = date_str[:3].title()\n",
        "        if weekday_str in weekdays:\n",
        "            weekday = weekdays.index(weekday_str)\n",
        "\n",
        "    # Date\n",
        "    match = re.search(r'(\\d{1,2})\\s+([a-z]{3})\\s+(\\d{4})', date_str)\n",
        "    if match:\n",
        "        day = int(match.group(1))\n",
        "        month = month_map.get(match.group(2))\n",
        "        year = int(match.group(3))\n",
        "\n",
        "    # Time\n",
        "    match_time = re.search(r'(\\d{2}):(\\d{2})(?::(\\d{2}))?', date_str)\n",
        "    if match_time:\n",
        "        hour = int(match_time.group(1))\n",
        "        minute = int(match_time.group(2))\n",
        "        second = int(match_time.group(3)) if match_time.group(3) else 0\n",
        "\n",
        "    #  2- Fallback (parsedate_to_datetime)\n",
        "    if None in [weekday, day, month, year, hour, minute, second]:\n",
        "        try:\n",
        "            dt = parsedate_to_datetime(date_str)\n",
        "            if dt:\n",
        "                weekday = weekday if weekday is not None else dt.weekday()\n",
        "                day = day if day is not None else dt.day\n",
        "                month = month if month is not None else dt.month\n",
        "                year = year if year is not None else dt.year\n",
        "                hour = hour if hour is not None else dt.hour\n",
        "                minute = minute if minute is not None else dt.minute\n",
        "                second = second if second is not None else dt.second\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    # 3- Safe defaults\n",
        "    weekday = 0 if weekday is None else weekday\n",
        "    day = 1 if day is None else day\n",
        "    month = 1 if month is None else month\n",
        "    year = 2025 if year is None else year\n",
        "    hour = 0 if hour is None else hour\n",
        "    minute = 0 if minute is None else minute\n",
        "    second = 0 if second is None else second\n",
        "\n",
        "    # Cyclical Encoding\n",
        "    def cyclical(val, max_val):\n",
        "        return (\n",
        "            np.sin(2 * np.pi * val / max_val),\n",
        "            np.cos(2 * np.pi * val / max_val)\n",
        "        )\n",
        "\n",
        "    weekday_sin, weekday_cos = cyclical(weekday, 7)\n",
        "    day_sin, day_cos = cyclical(day, 31)\n",
        "    month_sin, month_cos = cyclical(month, 12)\n",
        "    hour_sin, hour_cos = cyclical(hour, 24)\n",
        "    minute_sin, minute_cos = cyclical(minute, 60)\n",
        "    second_sin, second_cos = cyclical(second, 60)\n",
        "\n",
        "    return pd.Series([\n",
        "        weekday_sin, weekday_cos,\n",
        "        day_sin, day_cos,\n",
        "        month_sin, month_cos,\n",
        "        hour_sin, hour_cos,\n",
        "        minute_sin, minute_cos,\n",
        "        second_sin, second_cos,\n",
        "        year\n",
        "    ], index=[\n",
        "        'weekday_sin', 'weekday_cos',\n",
        "        'day_sin', 'day_cos',\n",
        "        'month_sin', 'month_cos',\n",
        "        'hour_sin', 'hour_cos',\n",
        "        'minute_sin', 'minute_cos',\n",
        "        'second_sin', 'second_cos',\n",
        "        'year'\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "1C_raw9Bi1Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Email Alert"
      ],
      "metadata": {
        "id": "-BF2lUB0vNny"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def send_alert_email(subject, body):\n",
        "    msg = EmailMessage()\n",
        "    msg[\"From\"] = EMAIL\n",
        "    msg[\"To\"] = EMAIL\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg.set_content(body)\n",
        "\n",
        "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
        "        server.login(EMAIL, APP_PASSWORD)\n",
        "        server.send_message(msg)\n"
      ],
      "metadata": {
        "id": "AncD1nbRguL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##FEATURE EXTRACTOR"
      ],
      "metadata": {
        "id": "ugSKSPhpvRS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_features(df):\n",
        "    features_df = df.copy()\n",
        "\n",
        "    subject_lower = features_df[\"subject\"].astype(str).str.lower()\n",
        "    body_lower = features_df[\"body\"].astype(str).str.lower()\n",
        "    from_lower = features_df[\"from\"].astype(str).str.lower()\n",
        "\n",
        "    url_col = \"url\" if \"url\" in features_df.columns else \"urls\"\n",
        "    urls_lower = features_df[url_col].fillna(\"\").astype(str).str.lower()\n",
        "    url_str_col = features_df[url_col].fillna(\"\").astype(str)\n",
        "\n",
        "    features_df[\"has_cc\"] = (~features_df[\"cc\"].isna()).astype(int)\n",
        "    features_df[\"has_bcc\"] = (~features_df[\"bcc\"].isna()).astype(int)\n",
        "    features_df[\"has_url\"] = (~features_df[url_col].isna()).astype(int)\n",
        "\n",
        "    features_df[\"cc_count\"] = features_df[\"cc\"].fillna(\"\").astype(str).apply(\n",
        "        lambda x: len([e for e in re.split(r\"[,;]\", x) if \"@\" in e])\n",
        "    )\n",
        "    features_df[\"bcc_count\"] = features_df[\"bcc\"].fillna(\"\").astype(str).apply(\n",
        "        lambda x: len([e for e in re.split(r\"[,;]\", x) if \"@\" in e])\n",
        "    )\n",
        "\n",
        "    features_df[\"total_recipients\"] = features_df[\"cc_count\"] + features_df[\"bcc_count\"] + 1\n",
        "    features_df[\"is_mass_email\"] = (features_df[\"total_recipients\"] > 5).astype(int)\n",
        "\n",
        "    features_df[\"subject_length\"] = features_df[\"subject\"].astype(str).apply(len)\n",
        "    features_df[\"subject_word_count\"] = features_df[\"subject\"].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    features_df[\"subject_urgent_keywords\"] = subject_lower.apply(\n",
        "        lambda x: sum(1 for kw in URGENT_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"subject_threat_keywords\"] = subject_lower.apply(\n",
        "        lambda x: sum(1 for kw in THREAT_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"subject_money_keywords\"] = subject_lower.apply(\n",
        "        lambda x: sum(1 for kw in MONEY_PRIZE_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"subject_generic_keywords\"] = subject_lower.apply(\n",
        "        lambda x: sum(1 for kw in GENERIC_SUSPICIOUS if kw in x)\n",
        "    )\n",
        "    features_df[\"subject_company_keywords\"] = subject_lower.apply(\n",
        "        lambda x: sum(1 for kw in COMPANY_IMPERSONATION if kw in x)\n",
        "    )\n",
        "\n",
        "    features_df[\"subject_excessive_punctuation\"] = features_df[\"subject\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"!{2,}|\\?{2,}|\\${2,}\", x)))\n",
        "    )\n",
        "    features_df[\"subject_exclamation_count\"] = features_df[\"subject\"].astype(str).apply(lambda x: x.count(\"!\"))\n",
        "    features_df[\"subject_question_count\"] = features_df[\"subject\"].astype(str).apply(lambda x: x.count(\"?\"))\n",
        "    features_df[\"subject_dollar_count\"] = features_df[\"subject\"].astype(str).apply(lambda x: x.count(\"$\"))\n",
        "\n",
        "    features_df[\"subject_all_caps_ratio\"] = features_df[\"subject\"].astype(str).apply(\n",
        "        lambda x: sum(1 for c in x if c.isupper()) / len(x) if len(x) > 0 else 0\n",
        "    )\n",
        "    features_df[\"subject_has_re_fwd\"] = subject_lower.apply(\n",
        "        lambda x: int(bool(re.match(r\"^(re:|fw:|fwd:)\", x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"sender_is_freemail\"] = from_lower.apply(\n",
        "        lambda x: int(any(dom in x for dom in FREE_EMAIL_DOMAINS))\n",
        "    )\n",
        "    features_df[\"sender_has_numbers\"] = features_df[\"from\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"\\d\", x)))\n",
        "    )\n",
        "    features_df[\"sender_number_count\"] = features_df[\"from\"].astype(str).apply(\n",
        "        lambda x: len(re.findall(r\"\\d\", x))\n",
        "    )\n",
        "    features_df[\"sender_length\"] = features_df[\"from\"].astype(str).apply(len)\n",
        "\n",
        "    features_df[\"sender_has_spoofing_indicator\"] = from_lower.apply(\n",
        "        lambda x: int(any(indicator in x for indicator in SPOOFING_INDICATORS))\n",
        "    )\n",
        "\n",
        "    features_df[\"sender_multiple_separators\"] = features_df[\"from\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"\\.{2,}|-{2,}|_{2,}\", x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_length\"] = features_df[\"body\"].astype(str).apply(len)\n",
        "    features_df[\"body_word_count\"] = features_df[\"body\"].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "    features_df[\"body_urgent_keywords\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in URGENT_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"body_threat_keywords\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in THREAT_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"body_money_keywords\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in MONEY_PRIZE_KEYWORDS if kw in x)\n",
        "    )\n",
        "    features_df[\"body_generic_keywords\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in GENERIC_SUSPICIOUS if kw in x)\n",
        "    )\n",
        "    features_df[\"body_company_keywords\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in COMPANY_IMPERSONATION if kw in x)\n",
        "    )\n",
        "    features_df[\"body_urgency_phrases\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for phrase in URGENCY_PHRASES if phrase in x)\n",
        "    )\n",
        "    features_df[\"body_credential_requests\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for kw in CREDENTIAL_REQUESTS if kw in x)\n",
        "    )\n",
        "\n",
        "    features_df[\"body_excessive_punctuation\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"!{2,}|\\?{2,}|\\${2,}\", x)))\n",
        "    )\n",
        "    features_df[\"body_exclamation_count\"] = features_df[\"body\"].astype(str).apply(lambda x: x.count(\"!\"))\n",
        "    features_df[\"body_question_count\"] = features_df[\"body\"].astype(str).apply(lambda x: x.count(\"?\"))\n",
        "    features_df[\"body_dollar_count\"] = features_df[\"body\"].astype(str).apply(lambda x: x.count(\"$\"))\n",
        "\n",
        "    features_df[\"body_has_html\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"<[^>]+>\", x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_has_misspellings\"] = body_lower.apply(\n",
        "        lambda x: int(any(word in x for word in common_misspellings))\n",
        "    )\n",
        "\n",
        "    features_df[\"url_count\"] = url_str_col.apply(\n",
        "        lambda x: len(re.findall(r\"https?://\", x))\n",
        "    )\n",
        "\n",
        "    shorteners = [\"bit.ly\" , \"tinyurl\" , \"goo.gl\" , \"t.co\" , \"ow.ly\" , \"is.gd\" , \"buff.ly\"]\n",
        "    features_df[\"url_is_shortened\"] = urls_lower.apply(\n",
        "        lambda x: int(any(short in x for short in shorteners))\n",
        "    )\n",
        "\n",
        "    features_df[\"url_avg_length\"] = url_str_col.apply(\n",
        "        lambda x: sum(len(url) for url in re.findall(r\"https?://[^\\s]+\", x)) / max(1 , len(re.findall(r\"https?://\", x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"url_many_subdomains\"] = url_str_col.apply(\n",
        "        lambda x: int(any(len(re.findall(r\"\\.\", url.split(\"/\")[2])) > 3\n",
        "                         for url in re.findall(r\"https?://[^\\s]+\", x) if \"/\" in url))\n",
        "    )\n",
        "\n",
        "    features_df[\"url_has_at_symbol\"] = url_str_col.apply(\n",
        "        lambda x: int(\"@\" in x and \"http\" in x)\n",
        "    )\n",
        "\n",
        "    features_df[\"sender_company_mismatch\"] = (\n",
        "        (features_df[\"sender_is_freemail\"] == 1) &\n",
        "        ((features_df[\"subject_company_keywords\"] > 0) |\n",
        "         (features_df[\"body_company_keywords\"] > 0))\n",
        "    ).astype(int)\n",
        "\n",
        "    features_df[\"total_urgent_keywords\"] = features_df[\"subject_urgent_keywords\"] + features_df[\"body_urgent_keywords\"]\n",
        "    features_df[\"total_threat_keywords\"] = features_df[\"subject_threat_keywords\"] + features_df[\"body_threat_keywords\"]\n",
        "    features_df[\"total_money_keywords\"] = features_df[\"subject_money_keywords\"] + features_df[\"body_money_keywords\"]\n",
        "    features_df[\"total_generic_keywords\"] = features_df[\"subject_generic_keywords\"] + features_df[\"body_generic_keywords\"]\n",
        "    features_df[\"total_company_keywords\"] = features_df[\"subject_company_keywords\"] + features_df[\"body_company_keywords\"]\n",
        "\n",
        "    features_df[\"urgent_keyword_ratio\"] = features_df[\"total_urgent_keywords\"] / (features_df[\"body_word_count\"] + 1)\n",
        "    features_df[\"threat_keyword_ratio\"] = features_df[\"total_threat_keywords\"] / (features_df[\"body_word_count\"] + 1)\n",
        "    features_df[\"money_keyword_ratio\"] = features_df[\"total_money_keywords\"] / (features_df[\"body_word_count\"] + 1)\n",
        "    features_df[\"generic_keyword_ratio\"] = features_df[\"total_generic_keywords\"] / (features_df[\"body_word_count\"] + 1)\n",
        "\n",
        "    features_df[\"url_to_text_ratio\"] = features_df[\"url_count\"] / (features_df[\"body_word_count\"] + 1)\n",
        "\n",
        "    features_df[\"is_reply_or_forward\"] = features_df[\"subject_has_re_fwd\"]\n",
        "\n",
        "    features_df[\"avg_word_length\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: sum(len(word) for word in x.split()) / max(1 , len(x.split()))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_uppercase_ratio\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: sum(1 for c in x if c.isupper()) / max(1 , len(x))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_special_char_count\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: len(re.findall(r\"[^a-zA-Z0-9\\s]\" , x))\n",
        "    )\n",
        "\n",
        "    features_df[\"mentions_attachment\"] = body_lower.apply(\n",
        "        lambda x: int(any(kw in x for kw in attachment_keywords))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_personal_pronoun_count\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for pronoun in personal_pronouns if pronoun in \" \" + x + \" \")\n",
        "    )\n",
        "\n",
        "    features_df[\"body_action_request_count\"] = body_lower.apply(\n",
        "        lambda x: sum(1 for action in action_words if action in x)\n",
        "    )\n",
        "\n",
        "    features_df[\"has_signature\"] = body_lower.apply(\n",
        "        lambda x: int(bool(re.search(r\"(regards|sincerely|best wishes|thanks|thank you)\" , x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_has_phone\"] = features_df[\"body\"].astype(str).apply(\n",
        "        lambda x: int(bool(re.search(r\"\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b\" , x)))\n",
        "    )\n",
        "\n",
        "    features_df[\"body_is_very_short\"] = (features_df[\"body_word_count\"] < 10).astype(int)\n",
        "\n",
        "    features_df[\"subject_body_overlap\"] = features_df.apply(\n",
        "        lambda row: len(set(str(row[\"subject\"]).lower().split()) &\n",
        "                       set(str(row[\"body\"]).lower().split())) /\n",
        "                   max(1 , len(str(row[\"subject\"]).split())),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    features_df[\"link_text_no_url\"] = (\n",
        "        (features_df[\"url_count\"] == 0) &\n",
        "        (body_lower.apply(lambda x: \"click\" in x or \"link\" in x or \"http\" in x))\n",
        "    ).astype(int)\n",
        "\n",
        "    return features_df\n"
      ],
      "metadata": {
        "id": "geOJKo_pglrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = extract_features(df)\n",
        "\n",
        "features_df[[\n",
        "    'weekday_sin','weekday_cos',\n",
        "    'day_sin','day_cos',\n",
        "    'month_sin','month_cos',\n",
        "    'hour_sin','hour_cos',\n",
        "    'minute_sin','minute_cos',\n",
        "    'second_sin','second_cos',\n",
        "    'year'\n",
        "]] = features_df['date'].apply(parse_date_text)\n",
        "\n",
        "features_df"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z60RK80U6_sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Phishing Detection Pipeline"
      ],
      "metadata": {
        "id": "o37D3epavkau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PhishingDetectionPipeline:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.tfidf_subject = None\n",
        "        self.tfidf_body = None\n",
        "        self.scaler = None\n",
        "        self.svd = None\n",
        "        self.kmeans = None\n",
        "        self.model = None\n",
        "        self.subject_weight = 2\n",
        "        self.numeric_features = [\n",
        "            \"subject_length\", \"subject_word_count\", \"body_length\", \"body_word_count\",\n",
        "            \"cc_count\", \"bcc_count\", \"total_recipients\", \"sender_length\", \"sender_number_count\",\n",
        "            \"subject_exclamation_count\", \"subject_question_count\", \"subject_dollar_count\",\n",
        "            \"body_exclamation_count\", \"body_question_count\", \"body_dollar_count\", \"avg_word_length\",\n",
        "            \"url_count\", \"url_avg_length\", \"total_urgent_keywords\", \"total_threat_keywords\",\n",
        "            \"total_money_keywords\", \"total_generic_keywords\", \"total_company_keywords\"\n",
        "        ]\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def fit(self, tfidf_subject, tfidf_body, scaler, svd, kmeans, model):\n",
        "        \"\"\"Store all fitted components\"\"\"\n",
        "        self.tfidf_subject = tfidf_subject\n",
        "        self.tfidf_body = tfidf_body\n",
        "        self.scaler = scaler\n",
        "        self.svd = svd\n",
        "        self.kmeans = kmeans\n",
        "        self.model = model\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        return self\n",
        "\n",
        "    def transform_features(self, df):\n",
        "\n",
        "        # TF-IDF for subject\n",
        "        X_subject = self.tfidf_subject.transform(df[\"subject\"])\n",
        "        X_subject_weighted = X_subject * self.subject_weight\n",
        "        df_subject_tfidf = pd.DataFrame(\n",
        "            X_subject_weighted.toarray(),\n",
        "            columns=[f\"subj_{w}\" for w in self.tfidf_subject.get_feature_names_out()],\n",
        "            index=df.index\n",
        "        )\n",
        "\n",
        "        # TF-IDF for body\n",
        "        X_body = self.tfidf_body.transform(df[\"body\"])\n",
        "        df_body_tfidf = pd.DataFrame(\n",
        "            X_body.toarray(),\n",
        "            columns=[f\"body_{w}\" for w in self.tfidf_body.get_feature_names_out()],\n",
        "            index=df.index\n",
        "        )\n",
        "\n",
        "        # Combine TF-IDF features\n",
        "        X_tfidf = pd.concat([df_subject_tfidf, df_body_tfidf], axis=1)\n",
        "\n",
        "        # Add link_text_no_url to TF-IDF (it was included during training)\n",
        "        if 'link_text_no_url' in df.columns:\n",
        "            X_tfidf.insert(0, 'link_text_no_url', df['link_text_no_url'].values)\n",
        "        else:\n",
        "            X_tfidf.insert(0, 'link_text_no_url', 0)\n",
        "\n",
        "        # Match SVD's expected features\n",
        "        if hasattr(self.svd, 'feature_names_in_'):\n",
        "            expected_features = list(self.svd.feature_names_in_)\n",
        "\n",
        "            # Add any other missing columns with zeros\n",
        "            for col in expected_features:\n",
        "                if col not in X_tfidf.columns:\n",
        "                    X_tfidf[col] = 0\n",
        "\n",
        "            # Keep only expected features in the right order\n",
        "            X_tfidf = X_tfidf[expected_features]\n",
        "\n",
        "        # Apply SVD to TF-IDF features -> produces 100 components\n",
        "        X_tfidf_reduced = self.svd.transform(X_tfidf)\n",
        "\n",
        "        # Now handle \"X_other\" features (all non-TF-IDF features)\n",
        "        # During training, you took ALL columns except TF-IDF columns\n",
        "        # Get what K-Means expects to figure out what was in X_other during training\n",
        "        if hasattr(self.kmeans, 'feature_names_in_'):\n",
        "            kmeans_features = list(self.kmeans.feature_names_in_)\n",
        "\n",
        "            print(f\"DEBUG: K-Means expects {len(kmeans_features)} total features\")\n",
        "\n",
        "            # Remove the SVD component names to get X_other column names\n",
        "            svd_cols = [f\"tfidf_svd_{i}\" for i in range(100)]\n",
        "            other_feature_names = [f for f in kmeans_features if f not in svd_cols]\n",
        "\n",
        "            print(f\"DEBUG: Extracting {len(other_feature_names)} 'other' features\")\n",
        "            print(f\"DEBUG: First 10 other features: {other_feature_names[:10]}\")\n",
        "            print(f\"DEBUG: Last 10 other features: {other_feature_names[-10:]}\")\n",
        "\n",
        "            # Extract these features from df\n",
        "            X_other = df.copy()\n",
        "\n",
        "            # Add missing features with zeros\n",
        "            missing_in_df = []\n",
        "            for feature in other_feature_names:\n",
        "                if feature not in X_other.columns:\n",
        "                    missing_in_df.append(feature)\n",
        "                    X_other[feature] = 0\n",
        "\n",
        "            if missing_in_df:\n",
        "                print(f\"DEBUG: Missing {len(missing_in_df)} features in df, added with zeros:\")\n",
        "                for feat in missing_in_df[:10]:\n",
        "                    print(f\"  - {feat}\")\n",
        "\n",
        "            # Select only the required features\n",
        "            X_other = X_other[other_feature_names].copy()\n",
        "\n",
        "            print(f\"DEBUG: X_other shape: {X_other.shape}\")\n",
        "\n",
        "            # Scale only the numeric_features that are in this subset\n",
        "            if hasattr(self.scaler, 'feature_names_in_'):\n",
        "                scalable_features = [f for f in self.scaler.feature_names_in_ if f in X_other.columns]\n",
        "            else:\n",
        "                scalable_features = [f for f in self.numeric_features if f in X_other.columns]\n",
        "\n",
        "            print(f\"DEBUG: Scaling {len(scalable_features)} features\")\n",
        "\n",
        "            X_other_scaled = X_other.copy()\n",
        "            if scalable_features:\n",
        "                X_other_scaled[scalable_features] = self.scaler.transform(X_other[scalable_features])\n",
        "        else:\n",
        "            # Fallback: use scaler features only\n",
        "            if hasattr(self.scaler, 'feature_names_in_'):\n",
        "                other_feature_names = list(self.scaler.feature_names_in_)\n",
        "            else:\n",
        "                other_feature_names = self.numeric_features\n",
        "\n",
        "            X_other = df[other_feature_names].copy()\n",
        "            X_other_scaled = X_other.copy()\n",
        "            X_other_scaled[other_feature_names] = self.scaler.transform(X_other[other_feature_names])\n",
        "\n",
        "        # Combine SVD-reduced TF-IDF with all other features\n",
        "        tfidf_cols = [f\"tfidf_svd_{i}\" for i in range(X_tfidf_reduced.shape[1])]\n",
        "        other_cols = X_other_scaled.columns.tolist()\n",
        "\n",
        "        X_combined = pd.DataFrame(\n",
        "            np.hstack([X_tfidf_reduced, X_other_scaled.values]),\n",
        "            index=df.index,\n",
        "            columns=tfidf_cols + other_cols\n",
        "        )\n",
        "\n",
        "        print(f\"DEBUG: After combining SVD + other features: {X_combined.shape[1]} features\")\n",
        "        print(f\"  - SVD components: {len(tfidf_cols)}\")\n",
        "        print(f\"  - Other features: {len(other_cols)}\")\n",
        "\n",
        "        # Final check: ensure we match K-Means expectations exactly\n",
        "        if hasattr(self.kmeans, 'feature_names_in_'):\n",
        "            expected_kmeans_features = list(self.kmeans.feature_names_in_)\n",
        "\n",
        "            print(f\"DEBUG: K-Means expects: {len(expected_kmeans_features)} features\")\n",
        "\n",
        "            # Add any still-missing columns with zeros\n",
        "            missing_cols = set(expected_kmeans_features) - set(X_combined.columns)\n",
        "            if missing_cols:\n",
        "                print(f\"DEBUG: Adding {len(missing_cols)} missing columns:\")\n",
        "                for col in list(missing_cols)[:10]:\n",
        "                    print(f\"  - {col}\")\n",
        "                if len(missing_cols) > 10:\n",
        "                    print(f\"  ... and {len(missing_cols) - 10} more\")\n",
        "\n",
        "                for col in missing_cols:\n",
        "                    X_combined[col] = 0\n",
        "\n",
        "            # Check for extra columns\n",
        "            extra_cols = set(X_combined.columns) - set(expected_kmeans_features)\n",
        "            if extra_cols:\n",
        "                print(f\"DEBUG: Removing {len(extra_cols)} extra columns:\")\n",
        "                for col in list(extra_cols)[:10]:\n",
        "                    print(f\"  - {col}\")\n",
        "                if len(extra_cols) > 10:\n",
        "                    print(f\"  ... and {len(extra_cols) - 10} more\")\n",
        "\n",
        "            # Reorder to match K-Means\n",
        "            X_combined = X_combined[expected_kmeans_features]\n",
        "\n",
        "        print(f\"DEBUG: Final shape before model: {X_combined.shape}\")\n",
        "\n",
        "        # Get cluster assignments\n",
        "        cluster_labels = self.kmeans.predict(X_combined)\n",
        "\n",
        "        return X_combined, cluster_labels\n",
        "\n",
        "    def predict(self, df, return_proba=False):\n",
        "\n",
        "        # Transform features\n",
        "        X_combined, cluster_labels = self.transform_features(df)\n",
        "\n",
        "        print(f\"DEBUG: X_combined shape after transform: {X_combined.shape}\")\n",
        "\n",
        "        # CRITICAL FIX: Add cluster labels as a feature for the model\n",
        "        # During training, cluster labels were added as a feature (181 + 1 = 182)\n",
        "        X_combined['cluster'] = cluster_labels\n",
        "\n",
        "        print(f\"DEBUG: X_combined shape after adding cluster: {X_combined.shape}\")\n",
        "\n",
        "        # Convert to tensor\n",
        "        X_tensor = torch.FloatTensor(X_combined.values).to(self.device)\n",
        "\n",
        "        # Get predictions\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(X_tensor)\n",
        "            probs = torch.sigmoid(outputs).cpu().numpy().flatten()\n",
        "\n",
        "        if return_proba:\n",
        "            return probs, cluster_labels\n",
        "        else:\n",
        "            predictions = (probs >= 0.5).astype(int)\n",
        "            return predictions, cluster_labels\n",
        "\n",
        "    def save(self, filepath):\n",
        "\n",
        "        pipeline_data = {\n",
        "            'tfidf_subject': self.tfidf_subject,\n",
        "            'tfidf_body': self.tfidf_body,\n",
        "            'scaler': self.scaler,\n",
        "            'svd': self.svd,\n",
        "            'kmeans': self.kmeans,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'model_class': self.model.__class__,\n",
        "            'subject_weight': self.subject_weight,\n",
        "            'numeric_features': self.numeric_features\n",
        "        }\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(pipeline_data, f)\n",
        "\n",
        "        print(f\"Pipeline saved to {filepath}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, filepath, model_instance=None):\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            pipeline_data = pickle.load(f)\n",
        "\n",
        "        pipeline = cls()\n",
        "        pipeline.tfidf_subject = pipeline_data['tfidf_subject']\n",
        "        pipeline.tfidf_body = pipeline_data['tfidf_body']\n",
        "        pipeline.scaler = pipeline_data['scaler']\n",
        "        pipeline.svd = pipeline_data['svd']\n",
        "        pipeline.kmeans = pipeline_data['kmeans']\n",
        "        pipeline.subject_weight = pipeline_data['subject_weight']\n",
        "        pipeline.numeric_features = pipeline_data['numeric_features']\n",
        "\n",
        "        # Load model\n",
        "        if model_instance is None:\n",
        "            raise ValueError(\"Please provide a model_instance with the same architecture\")\n",
        "\n",
        "        pipeline.model = model_instance\n",
        "        pipeline.model.load_state_dict(pipeline_data['model_state_dict'])\n",
        "        pipeline.model.to(pipeline.device)\n",
        "        pipeline.model.eval()\n",
        "\n",
        "        print(f\"Pipeline loaded from {filepath}\")\n",
        "        return pipeline\n",
        "\n",
        "\n",
        "#  USAGE SCRIPT\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Load the model (expects 182 input features)\n",
        "    model = PhishingDetector(182)\n",
        "\n",
        "    # Load the pipeline\n",
        "    pipeline = PhishingDetectionPipeline.load('phishing_detection_pipeline.pkl', model_instance=model)\n",
        "\n",
        "    print(\"Making predictions...\")\n",
        "\n",
        "    # Make predictions\n",
        "    # features_df should contain: subject, body, and all 90 features you showed earlier\n",
        "    predictions, clusters = pipeline.predict(features_df)\n",
        "\n",
        "    print(f\"\\nâœ“ Predictions completed successfully!\")\n",
        "    print(f\"  - Predictions shape: {predictions.shape}\")\n",
        "    print(f\"  - Cluster assignments shape: {clusters.shape}\")\n",
        "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
        "    print(f\"First 10 clusters: {clusters[:10]}\")\n",
        "\n",
        "    # Get probabilities\n",
        "    probabilities, _ = pipeline.predict(features_df, return_proba=True)\n",
        "    print(f\"\\nFirst 10 probabilities: {probabilities[:10]}\")\n",
        "\n",
        "    # Summary statistics\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    print(\"PREDICTION SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total emails analyzed: {len(predictions)}\")\n",
        "    print(f\"Predicted as phishing: {predictions.sum()} ({predictions.sum()/len(predictions)*100:.1f}%)\")\n",
        "    print(f\"Predicted as legitimate: {(predictions == 0).sum()} ({(predictions == 0).sum()/len(predictions)*100:.1f}%)\")\n",
        "    print(f\"\\nCluster distribution:\")\n",
        "    unique, counts = np.unique(clusters, return_counts=True)\n",
        "    for cluster_id, count in zip(unique, counts):\n",
        "        print(f\"  Cluster {cluster_id}: {count} emails ({count/len(clusters)*100:.1f}%)\")\n",
        "    print(\"=\"*70)"
      ],
      "metadata": {
        "id": "b3DOlLppgpF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Call The Model"
      ],
      "metadata": {
        "id": "Ou1Q3yi4vz6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in pipeline.numeric_features:\n",
        "    if col not in features_df.columns:\n",
        "        features_df[col] = 0\n",
        "\n",
        "\n",
        "ml_probs, clusters = pipeline.predict(\n",
        "    features_df,\n",
        "    return_proba=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "mNpLAabs7-Yk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run Detection"
      ],
      "metadata": {
        "id": "tzASnXGSv57S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIGH_THRESHOLD = 0.85\n",
        "MEDIUM_THRESHOLD = 0.6\n",
        "\n",
        "for i, row in features_df.iterrows():\n",
        "    ml_prob = ml_probs[i]\n",
        "\n",
        "\n",
        "    if ml_prob < MEDIUM_THRESHOLD:\n",
        "        continue\n",
        "\n",
        "\n",
        "    if ml_prob >= HIGH_THRESHOLD:\n",
        "        subject = \"ðŸš¨ HIGH RISK PHISHING EMAIL\"\n",
        "        risk_level = \"HIGH RISK\"\n",
        "    else:\n",
        "        subject = \"âš ï¸ SUSPICIOUS EMAIL DETECTED\"\n",
        "        risk_level = \"MEDIUM RISK\"\n",
        "\n",
        "    body = f\"\"\"\n",
        "From: {row['from']}\n",
        "Subject: {row['subject']}\n",
        "\n",
        "Phishing Probability: {ml_prob:.3f}\n",
        "Risk Level: {risk_level}\n",
        "Model Decision: POSSIBLE_PHISHING\n",
        "\"\"\"\n",
        "\n",
        "    send_alert_email(subject, body)\n"
      ],
      "metadata": {
        "id": "pUoQxe60gzKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTZ4AjDCg291"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
